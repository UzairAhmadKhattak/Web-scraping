#!/usr/bin/env python
# coding: utf-8

# ## Web Scraping(From Amazon)
# ## My scraper extract the data from Amazon
# ## The user have to just enter the item and number of pages from which he/she want to extract data.
# 

# ## Importing libraries

# In[1]:


from bs4 import BeautifulSoup

from selenium import webdriver

import pandas as pd


# In[2]:


driver = webdriver.Chrome(r'C:\Users\Uzair Ahmad\.wdm\drivers\chromedriver\win32\91.0.4472.101\chromedriver.exe')


# ## Getting Urls

# In[3]:


def get_url(item_search,pages):
    url_list = []
    url = 'https://www.amazon.com/s?k={}&page='
    item_search = item_search.strip()
    item_search = item_search.replace(' ','+')
    for page_num in range(pages):
        url_list.append(url.format(item_search)+str(page_num+1))

    return url_list


# ## This is for user
# ## Enter name of item and number of pages

# In[4]:


urls=get_url('laptop',1) #Enter item and number of pages


# In[5]:


urls


# ## Scraper

# In[15]:


def scraper():
    data=[]
    for url in urls:  # urls of different pages
        driver.get(url) # driver of first page
        soup=BeautifulSoup(driver.page_source,'html.parser') # soup for different pages
        results = soup.find_all("div",{"data-component-type":"s-search-result"}) # content of different pages
        for item in range(len(results)): # looping through a page to get all items
            dic={}
            
            
            item_url='https://www.amazon.com'+results[item].a.get('href') # url of an item page
            driver.get(item_url) #driver for an item contents
            soup1=BeautifulSoup(driver.page_source,'html.parser') # soup for an item page
            item_specs_heading = soup1.find_all('th',{'class':'a-color-secondary a-size-base prodDetSectionEntry'})
            item_specs = soup1.find_all('td',{'class':'a-size-base prodDetAttrValue'}) # getting content of item page

            for num in range(len(item_specs)):
                try:
                    dic[item_specs_heading[num].text.strip()] = item_specs[num].text.strip().replace('\u200e','')
                except:
                    dic[item_specs_heading[num].text.strip()] = None

                
            try:
                dic['Price'] = soup1.find('span',{'id':'priceblock_ourprice'}).text
            except:
                dic['Price']=None
            
            try:
                dic["Ratings"] = soup1.find('span',{'id':'acrCustomerReviewText'}).text
            except:
                dic["Ratings"] = None
            
            try:
                dic["Reviews"] = soup1.find('span',{'class':'a-icon-alt'}).text
            except:
                dic["Reviews"] = None
                              
            try:
                dic["Reviews Link"] = 'https://www.amazon.com'+soup1.find('a',{'class':'a-link-normal'}).get('href')
            except:
                dic["Reviews Link"] = None
                
            dic = list(dic.items())
            dic.sort()
            dic = dict(dic)
            data.append(dic)
    driver.close()
    return data


# ## Making data frame from data extracted data

# In[16]:


extracted_data=scraper()


# In[17]:


extracted_data


# In[18]:


df=pd.DataFrame(extracted_data)
df


# In[20]:


df.to_csv('AmazonLaptopExtractedData.csv')

